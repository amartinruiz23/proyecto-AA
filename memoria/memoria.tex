\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{caption}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{boldline}
\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{hyperref}

% Meta
\title{Práctica 3}
\author{Pedro Bonilla Nadal}
\date{\today}

% Custom
\providecommand{\abs}[1]{\lvert#1\rvert}
\setlength\parindent{0pt}
\definecolor{Light}{gray}{.90}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
% Primera derivada parcial: \pder[f]{x}
\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}

\begin{document}
\begin{titlepage}
\begin{center}

\vspace*{.06\textheight}
{\scshape\LARGE Universidad De Granada\par}\vspace{1.5cm} % University name
\textsc{\Large Apredizaje Automático}\\[0.5cm] % Thesis type

\rule{\textwidth}{0.4mm} \\[0.4cm] % Horizontal line
{\huge \bfseries Proyecto Final\par}\vspace{0.4cm} % Thesis title

\rule{\textwidth}{0.4mm} \\[11.5cm] % Horizontal line
 {\Large Pedro Bonilla Nadal\\Antonio Martín Ruiz}\\[1cm]

 {\today}

\vfill
\end{center}
\end{titlepage}

\setcounter{tocdepth}{2}
\tableofcontents
\newpage



\section{Compensión del problema a resolver }

Este dataset contiene datos de la oficina del censo\cite{census} relativos a censos de 1995, obtenidos del repositorio UCI de bases de datos \cite{uci}. Un total de 48842 personas han sido encuestadas para este censo. De estas personas tenemos una serie de variables con información de carácter socioeconómico. En particular:
\begin{itemize}
\item tenemos 6 variables de tipo numérico u ordinal, con valores en rangos distintos.
\item 8 variables de tipo categórico.
\item Una variable de clase que toma como valores $<50$K y $>50$k.
\item Es un problema de clasificación, ya que no tenemos información para hacer una regresión sobre la ganancia.
\end{itemize}

El código utilizado para la resolución de la práctica se encuentra en el archivo \texttt{main.py}.  Del mismo modo, los datos limpiados se encuentran en el archivo \texttt{adults.data} y su información relativa procesada en el archivo \texttt{adult.names}.


\section{ División y codificación de los datos}

A la hora de obetener los datos en el respositorio se encuentran tres archivos: \texttt{adult.data},  \texttt{adult.test} y \texttt{adult.names}. Estos datos fueron procesados en el año 1996. Nosotros realizamos una operación de formato de los datos para ajustarlos a convenciones más recientes.

\begin{itemize}
\item Cambiamos la variable de clase por valores categóricos 0 para $<50$K y 1 para $>50$K
\item Eliminamos el espaciado después de la coma para facilitar la lectura por parte de librerías actuales.
\item Añadimos la información del archivo \texttt{adult.test} al archivo \texttt{adult.data} con el objetivo de tener la información procesada de manera conjunta.
\item Añadimos el flag \texttt{@attribute} a la lista de atributos provista en \texttt{adult.names}.
\end{itemize}

Una vez leidos los datos haremos una division train/test de 80\%/20\%. Elegimos esta proporción aprovechando que dada la gran cantidad de datos de los que poseemos. Obtenemos por lo tanto un conjunto de train con 39074 instancias, así como un conjuto de test con 9768 instancias.\\

Además, a la hora de validar la selección de hiperparámetros utilizaremos la técnica cross validation, para la cual diviremos el conjunto de training en 5 subconjuntos. Se utilizará para esta la función \texttt{cross\_validate}\cite{cv}.\\

Como ultimo detalle de codificación de los datos, durante la parte de preprocesado realizamos la técnica de dummy variables. Esta técnica permite entender como variables categóricas 




\section{Preprocesado}

Mostraremos a continuación las siguientes técnicas de preprocesado realizadas. Justificaremos individualmente su uso.

\begin{itemize}

	\item \underline{Normalización de las variables}: realizaremos una normalización de los datos, para evitar que su escala afecte a la relevancia de estos. Para ello usaremos la función \texttt{StandardScaler}\cite{standardscaler} provista en la librería \texttt{sklearn}.
	\item \underline{Aumento de dimensionalidad}: debido a la baja cantidad de variables, en contraposicición con la alta cantidad de ejemplos podemos considerar aumentos de la dimensionalidad de los datos sin miedo a que se genere un gran sobreajuste.
	\begin{itemize}
	\item[-] Dummy Variables: una variable dummy es aquella que toma sólo el valor 0 o 1 para indicar la ausencia o la presencia de algún efecto categórico que se pueda esperar que cambie el resultado. Esta técnica ayudará a codificar la entrada para algunos algoritmos, así como poder hacer un estudio de cada categoría como una variable separada a la hora de hacer una valoración del interés de cada variable.\\
	
	Incluir inofrmación de como se ha realizado.
	 
	\item[-] Polynomial Freatures: con especial interés en el caso lineal, el uso de variaciónes polinómicas de los datos puede ser util para permitir aproximaciones a clases de funciones nuevas. Probaremos variación cuadrática sobre las variables numéricas cuando realizemos un ajuste lineal. Para Random Forest no lo consideramos necesario por la ya conocida complejidad de los arboles, y para SVM utilizaremos variaciones de kernell. \\
	
	Incluir información de como se ha realizado
\end{itemize}
	\item \underline{Valoración de las variables de interés}: Para realizar un estudio de las variables de interés 
	\item \underline{Datos incompletos y valores perdidos}: en relación a los valores perdidos encontramos en tres variables de tipo categórico.\\
	
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|ll|}
\hline
Variable& Valores distintos & Valores Perdidos\\ \hline
\texttt{workclass} & 9 & 2799\\
\texttt{occupation} & 15 & 2809\\
\texttt{native-country} & 42 & 857\\\hline
\end{tabular}
\end{center}
\caption{Representación de valores perdidos.}
	\end{table}
	
	Realizamos la sustitución de estos datos mediante la función \texttt{replace\_lost\_categorical\_values}. En esta, para cada columna, calculamos su distribución de probabilidad, esto es, sumamos las ocurrencias de cada categoría y dividimos entre el total de valores con valores no perdidos. Obtenemos así la probabilidad de cada categoría. A continuación calculamos las probabilidades acumuladas sumando para categoría su probablidad y la de todas las anteriores. Para cada dato perdido generamos un número aleatorio mediante una distribución uniforme en el intervalo $[0,1]$. La clase por la que el valor perdido será sustituida será la de cuyo intervalo contenga al valor generado, siendo el intervalo de cada clase el comprendido entre la probabilidad acumulada de la anterior y su probabilidad acumulada (incluyendo en cada uno su extremo inferior, pero no su extremo superior).
	
	\item \underline{Datos inconsistentes}: no encontramos datos inconsistentes.
	\item \underline{Balanceo de clases}: nos encontramos ante una situación con un desbalanceo notable. Sin embargo, dada la cantidad de datos provista por la base de datos creemos que no es necesario realizar modificaciones de los datos ni en el conjunto de train ni en el de test.
\end{itemize}




\section{ Métricas }

Hemmmos decidido conseiderar, en este contexto de clasificación, dos funciones con objetivo de medir el error, ambas halladas en \cite{met}.
\begin{itemize}
	\item \emph{accuracy}: Decidimos utilizar esta medida por su simplicidad y expresividad. Esta métrica nos propocionará una idea general de la bondad de nuestro modelo en un caso general, así como nos permitirá comparar, por ejemplo, con el clasificador modal. 
	\item \emph{f1-score}: Al estar en un modelo de clases desbalanceadas, consideramos que debiamos utilizar una métrica que penara comportamientos de 'asignación modal'. La medida $F_1$ se define como\cite{f1}:
	$${\displaystyle F_{1}={\frac {2}{\mathrm {recall} ^{-1}+\mathrm {precision} ^{-1}}}=2\cdot {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}}.$$
	
	Donde precision es (en terminos de clase positiva/negativa) el número de verdaderos positivos entre los positivos escogidos, y recall es el número de positivos escogidos entre los positivos totales.
\end{itemize}
\section{ Modelos Considerados }

\subsection{ Modelo Lineal }
	
	Como modelo lineal, dado que estamos en clasificación, podemos escoger de los modelos explicados en clase entre Regresión Logística y  PLA. Decidimos utilizar regresión logística por una serie de motivos.
	
	\begin{itemize}
	\item Al estar en un dataset que proviene de un censo sucede que encontraremos una gran cantidad de outliers y datos algo ruidosos. Esto hará, que el perceptron sea contraindicado, dado que dificilmente podremos conseguir convergencia de este debido al ruido, y en el caso de que uno de estos elementos ruidosos sea además un outlier podría variar notable (e indebidamente) el plano clasificador).
	\item Nos resulta interesante el hecho de que podamos obtener la confianza de una predicción y no solo la predicción.
\end{itemize}

Para el uso de la regresión logistica haremos uso de la función de SKLearn 

\subsection{Random Forest}

\subsection{SVM}

Consideramos que la técnica SVM-soft margin puede ser muy útil para este problema debido a que podemos fijar a placer el grado de compromiso entre busqueda del plano óptimo y la tolerancia al ruido, ambas cosas resultando de mucha utilidad a la hora de trabajar con datos potencialmente ruidosos.\\

Otro motivo para decidir ajustar este modelo, aunque de carácter menos importante, es la curiosidad suscitada por este modelo durante el desarrollo teórico de la asignatura. 

\section{ Técnica de Ajuste para el Modelo Lineal}
\subsection{Regresión Logística}

Para este conjunto utilizaremos dos técinas de ajuste, ambas definidas como variaciones especializadas sobre el algoritmo SGD.:
\begin{itemize}
	\item \texttt{lbfgs}: Utiliza una variación del método Newton visto en el bonus de la práctica 1 que en lugar de la matriz Hessiana utiliza una aproximación a esta. Es eficiente en comparación con el método de newton estandard y se podría definir como una variación del gradiente descendente estocástico. Como desventajas tiene que se podría acercar a puntos críticos que no fuese mínimos. Esta técnica no admite regularización \texttt{l1}.
	\item \texttt{liblinear}: es el ganador ICML 2008 large-scale learning challenge. Lo utilizamos para probar las regularizaciónes dado que funciona tanto con \texttt{l1} como con \texttt{l2}.
	
	Liblinear utiliza técnicas de descenso coordinado\cite{CD} donde minimizamos la función en torno a cada eje, resolviendo un problema de minimización unidimensional en bucle. 
\end{itemize}

Estas dos técnicas se pueden entender como variaciones del gradiente descendiente estocastico en esecia, dado que surgen de su filosofía de realizar aproximaciones sucesivas al mínimo. Pese a ello se debe comprender que se parecen en poco más que filosofía. Son técnicas mucho más eficientes que el algoritmo de la pseudoinversa, cosa que nos puede resultar interesante, aunque trabajamos con un dataset que no es particularmente grande. Las compararemos y comentaremos sus resultados  en la sección de hiperparámetros y selección del modelo, pero elegiremos lbfgs dado que para el tamaño de nuestro dataset no es relevante la mejora en eficiencia de \texttt{liblinear} y \texttt{lbfgs} consigue un resultado ligeramente mejor.


\section{Hiperparámetros y selección del modelo}
 Aplicación de la técnica especificando claramente que algoritmos se usan en la estimación de los parámetros, los hiperparámetros y el error de generalización

\section{ Error de Generalización}
\section{ Argumentar sobre la idoneidad de la función regularización usada }
\section{ Conclusiónes }
Valoración de los resultados y justificación
( gráficas, métricas de error, análisis de residuos, etc )


que se ha obtenido la mejor de las posibles soluciones con la técnica elegida y la muestra dada. Argumentar en términos de los errores de ajuste y generalización. 


\newpage
\begin{thebibliography}{9}
\bibitem{census}
Página oficial de la Oficina del censo: \url{http://www.census.gov/ftp/pub/DES/www/welcome.html}.

\bibitem{uci}
Página del Centro para el Apredizaje Automático y Sistemas Inteligentes:\url{http://archive.ics.uci.edu/ml/datasets/Adult}.

\bibitem{standardscaler}
Función \texttt{StandardScaler} de la librería \texttt{sklearn}: \url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}

\bibitem{cv}
Función \texttt{cross\_validate} de la librería \texttt{sklearn}:
\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}

\bibitem{dummy var}
Artículo donde explica el concepto de dummy variable: \url{https://en.wikipedia.org/wiki/Dummy_variable_(statistics)}

\bibitem{met}
Métricas de sklearn \url{https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score}

\bibitem{f1}

Métrica f1-score \url{https://en.wikipedia.org/wiki/F1_score}


\bibitem{CD}
Código RL: \url{https://en.wikipedia.org/wiki/Coordinate_descent}

\end{thebibliography}





\end{document}
