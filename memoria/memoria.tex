\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{caption}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{boldline}
\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{hyperref}

% Meta
\title{Práctica 3}
\author{Pedro Bonilla Nadal}
\date{\today}

% Custom
\providecommand{\abs}[1]{\lvert#1\rvert}
\setlength\parindent{0pt}
\definecolor{Light}{gray}{.90}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
% Primera derivada parcial: \pder[f]{x}
\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}

\begin{document}
\begin{titlepage}
\begin{center}

\vspace*{.06\textheight}
{\scshape\LARGE Universidad De Granada\par}\vspace{1.5cm} % University name
\textsc{\Large Apredizaje Automático}\\[0.5cm] % Thesis type

\rule{\textwidth}{0.4mm} \\[0.4cm] % Horizontal line
{\huge \bfseries Proyecto Final\par}\vspace{0.4cm} % Thesis title

\rule{\textwidth}{0.4mm} \\[11.5cm] % Horizontal line
 {\Large Pedro Bonilla Nadal\\Antonio Martín Ruiz}\\[1cm]

 {\today}

\vfill
\end{center}
\end{titlepage}

\setcounter{tocdepth}{2}
\tableofcontents
\newpage



\section{Compensión del problema a resolver }

Este dataset contiene datos de la oficina del censo\cite{census} relativos a censos de 1995, obtenidos del repositorio UCI de bases de datos \cite{uci}. Un total de 48842 personas han sido encuestadas para este censo. De estas personas tenemos una serie de variables con información de carácter socioeconómico. En particular:
\begin{itemize}
\item tenemos 6 variables de tipo numérico y entero, con valores en rangos distintos.
\item 8 variables de tipo categórico.
\item Una variable de clase que toma como valores $<50$K y $>50$k.
\item Es un problema de clasificación, ya que no tenemos información para hacer una regresión sobre la ganancia.
\end{itemize}

El código utilizado para la resolución de la práctica se encuentra en el archivo \texttt{main.py}.  Del mismo modo, los datos limpiados se encuentran en el archivo \texttt{adults.data} y su información relativa procesada en el archivo \texttt{adult.names}.


\section{ División y codificación de los conjuntos}

A la hora de obetener los datos en el respositorio se encuentran tres archivos: \texttt{adult.data},  \texttt{adult.test} y \texttt{adult.names}. Estos datos fueron procesados en el año 1996. Nosotros realizamos una operación de formato de los datos para ajustarlos a convenciones más recientes.

\begin{itemize}
\item Cambiamos la variable de clase por valores categóricos 0 para $<50$K y 1 para $>50$K
\item Eliminamos el espaciado después de la coma para facilitar la lectura por parte de librerías actuales.
\item Añadimos la información del archivo \texttt{adult.test} al archivo \texttt{adult.data} con el objetivo de tener la información procesada de manera conjunta.
\item Añadimos el flag \texttt{@attribute} a la lista de atributos provista en \texttt{adult.names}.
\end{itemize}

Una vez leidos los datos haremos una division train/test de 80\%/20\%. Elegimos esta proporción aprovechando que dada la gran cantidad de datos de los que poseemos. Obtenemos por lo tanto un conjunto de train con 39074 instancias, así como un conjuto de test con 9768 instancias.\\

Además, a la hora de validar la selección de hiperparámetros utilizaremos la técnica cross validation, para la cual diviremos el conjunto de training en 5 subconjuntos. Se utilizará para esta la función \texttt{cross\_validate}\cite{cv}.




\section{Preprocesado}

Mostraremos a continuación las siguientes técnicas de preprocesado realizadas. Justificaremos individualmente su uso.

\begin{itemize}

	\item \underline{Normalización de las variables}: realizaremos una normalización de los datos, para evitar que su escala afecte a la relevancia de estos. Para ello usaremos la función \texttt{StandardScaler}\cite{standardscaler} provista en la librería \texttt{sklearn}.
	\item Aumento de dimensionalidad: debido a la baja cantidad de variables, en contraposicición con la alta cantidad de ejemplos podemos considerar aumentos de la dimensionalidad de los datos sin miedo a que se genere un gran sobreajuste.
	\begin{itemize}
	\item[-] Dummy Variables: una variable dummy es aquella que toma sólo el valor 0 o 1 para indicar la ausencia o la presencia de algún efecto categórico que se pueda esperar que cambie el resultado. Esta técnica ayudará a codificar la entrada para algunos algoritmos, así como poder hacer un estudio de cada categoría como una variable separada a la hora de hacer una valoración del interés de cada variable.\\
	
	Incluir inofrmación de como se ha realizado.
	 
	\item[-] Polynomial Freatures: con especial interés en el caso lineal, el uso de variaciónes polinómicas de los datos puede ser util para permitir aproximaciones a clases de funciones nuevas. Probaremos variación cuadrática sobre las variables numéricas cuando realizemos un ajuste lineal. Para Random Forest no lo consideramos necesario por la ya conocida complejidad de los arboles, y para SVM utilizaremos variaciones de kernell. \\
	
	Incluir información de como se ha realizado
\end{itemize}
	\item Valoración de las variables de interés: 
	\item Datos incompletos y valores perdidos: en relación a los valores perdidos encontramos solo de tipo categórico.\\
	
	#   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   age             48842 non-null  int64 
 1   workclass       46043 non-null  object
 2   fnlwgt          48842 non-null  int64 
 3   education       48842 non-null  object
 4   education-num   48842 non-null  int64 
 5   marital-status  48842 non-null  object
 6   occupation      46033 non-null  object
 7   relationship    48842 non-null  object
 8   race            48842 non-null  object
 9   sex             48842 non-null  object
 10  capital-gain    48842 non-null  int64 
 11  capital-loss    48842 non-null  int64 
 12  hours-per-week  48842 non-null  int64 
 13  native-country  47985 non-null  object
 14  Class           48842 non-null  object
	Tabla con los valores perdidos\\
	
	Los sustituimos haciendo esto.
	\item Datos inconsistentes: no encontramos clases inconsistentes.
\end{itemize}




\section{ Función de pérdida usada}
\section{ Técnica de Ajuste para el Modelo Lineal}
Selección de las técnica (parámetrica) y valoración de la idoneidad de la misma frente a otras alternativas
\section{Hiperparámetros y selección del modelo}
 Aplicación de la técnica especificando claramente que algoritmos se usan en la estimación de los parámetros, los hiperparámetros y el error de generalización

\section{ Error de Generalización}
\section{ Argumentar sobre la idoneidad de la función regularización usada }
\section{ Conclusiónes }
Valoración de los resultados y justificación
( gráficas, métricas de error, análisis de residuos, etc )


que se ha obtenido la mejor de las posibles soluciones con la técnica elegida y la muestra dada. Argumentar en términos de los errores de ajuste y generalización,


\newpage
\begin{thebibliography}{9}
\bibitem{census}
Página oficial de la Oficina del censo: \url{http://www.census.gov/ftp/pub/DES/www/welcome.html}.

\bibitem{uci}
Página del Centro para el Apredizaje Automático y Sistemas Inteligentes:\url{http://archive.ics.uci.edu/ml/datasets/Adult}.

\bibitem{standardscaler}
Función \texttt{StandardScaler} de la librería \texttt{sklearn}: \url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}

\bibitem{cv}
Función \texttt{cross\_validate} de la librería \texttt{sklearn}:
\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html}

\bibitem{dummy var}
Artículo donde explica el concepto de dummy variable: \url{https://en.wikipedia.org/wiki/Dummy_variable_(statistics)}
\end{thebibliography}




\end{document}
